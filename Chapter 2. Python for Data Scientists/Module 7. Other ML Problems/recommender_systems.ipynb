{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Introduction: Simple Recommender Systems](#Introduction:-Simple-Recommeder-Systems)\n",
    "- [Offline & Online Evaluation](#Offline-&-Online-Evaluation)\n",
    "- [Content-based Recommenders](#Content\\-based-Recommenders)\n",
    "- [Collaborative-filtering](#Collaborative\\-filtering)\n",
    "- [Hybrid Systems](#Hybrid-Systems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Simple Recommender Systems\n",
    "\n",
    "__Recommender systems__, also referred to as __recommendation systems__, are filtering systems used by many different companies world-wide to be able to recommend products (e.g. movies, clothes, etc) based on user preferences. Unlike __ranking algorithms,__ recommender systems aim to provide recommendations without an explicit input from the user (such as a search query). We obviously cannot recommend _exactly_ what a user wants as we cannot access or process all the information in their brain at the same time. Instead, we can take advantage or users' past ratings, choices and preferences to __predict__ the products the user will most probably like.\n",
    "\n",
    "How do these systems do what they do? This is question that has become a large topic of research and the current answer is that there are mutliple ways to create recommender systems: each working under different assumptions and algorithms. There are two main broad classifications that we will cover shortly: __content-based recommendation__ (item-centred) and __collaborative filtering__ (user-centred).\n",
    "\n",
    "<img width=\"500px;\" height=\"500px\" src=\"https://www.researchgate.net/profile/Lionel_Ngoupeyou_Tondji/publication/323726564/figure/fig5/AS:631605009846299@1527597777415/Content-based-filtering-vs-Collaborative-filtering-Source.png\">\n",
    "\n",
    "Before we cover the implementation of these, we will cover what is informally referred to as a simple recommender system: a system that uses the weighted average rating from all users to make recommendations on the \"best\" options. This is also referred to as a type of __demographic recommender system.__ Throughout this notebook, we will use the \"Movies Dataset\" from [Kaggle](https://www.kaggle.com/rounakbanik/the-movies-dataset), where the full version contains information on over 45,000 movies with 26 million ratings from  270,000 users. We will be using the small version, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ratings\n",
    "ratings = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing movie metadata\n",
    "metadata = pd.read_csv(\"../DATA/movies_metadata.csv\")\n",
    "# metadata.head()\n",
    "metadata.nlargest(n=10, columns=[\"vote_average\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple recommender system, we will use the IMDB's known __weighted average formula__ used for their Top Movies Chart, given as follows:\n",
    "\n",
    "$$ R_{W} = (\\frac{v}{v + m})R + (\\frac{m}{v + m})C  $$\n",
    "\n",
    "Where:\n",
    "- $R_{W}$ is the weighted average movie rating\n",
    "- $v$ is the number of votes for that movie title\n",
    "- $m$ is the minimum number of votes required to be in the top Chart\n",
    "- $R$ is the average rating of that movie title\n",
    "- $C$ is the mean vote rating across all movies\n",
    "\n",
    "We can now begin our calculations to construct our simple recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean vote count across all movies\n",
    "vote_counts = metadata[metadata['vote_count'].notnull()]['vote_count'].astype('int')\n",
    "vote_averages = metadata[metadata['vote_average'].notnull()]['vote_average'].astype('int')\n",
    "C = vote_averages.mean()\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now choose a value for the minimum number. In this case, we will choose a value $m$ that gives us movies that have received more votes than 95% of the other remaining movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Computing minimum number of votes required\n",
    "m = vote_counts.quantile(0.95)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the movies that are considered to be canditates for the top charts in our recommender system given our computed 'm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all movies that have a votecount that is greater than our m value\n",
    "qualified = metadata[(metadata['vote_count'] >= m) & (metadata['vote_count'].notnull()) & (metadata['vote_average'].notnull())] \\\n",
    "                 [['title', 'release_date', 'vote_count', 'vote_average', 'popularity', 'genres']]\n",
    "qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
    "qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
    "qualified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing weighted average and determining top 250 chart\n",
    "def weighted_rating(x):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "qualified['weighted_average'] = qualified.apply(weighted_rating, axis=1)\n",
    "qualified = qualified.sort_values('weighted_average', ascending=False).head(250)\n",
    "qualified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now we have the top charts. This chart can be carried further to become a simple recommender system by recommending the top movies in the charts to all users. Is this a good recommender system? Not particularly. By taking a global weighted average we are able to determine which ones are considered the best on average, but we are unable to account for the individual preferences of the users. For instance, if I was a fan of exclusively Romcoms, I would be recommended only movies I dislike from the list above. The same would happen to people who really don't like action and thrillers. Therefore, we must use our data to instead be able to account for individual preferences! Therefore, simple recommender systems like this that generate global recommendations are generally only used for users who the system has collected little data from.\n",
    "\n",
    "By accounting for individual user preferences, we would likely achieve a higher score. But how can we determine which system is better? This leads us to the two methods of recommender system evaluation: __offline evaluation__ and __online evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline & Online Evaluation\n",
    "\n",
    "How can we tell that our recommender system is doing what it is supposed to? There are two different approaches to evaluating our system:\n",
    "\n",
    "- __Offline evaluation:__ Use data we already have and evaluation metrics to compute numeric efectiveness measures that can be tuned for and/or compared. These are the same evaluation metrics which we have encountered and used to assess the performance of our models\n",
    "- __Online evaluation:__  involves using a live system, and tracking user-related behaviors such as dwell-times, click-through rates, and purchase conversions\n",
    "\n",
    "When carrying offline evaluation, we can split our data into a training and a test dataset just as we have seen before to ensure that we are tuning our systems appropriately. On the other hand, online evaluation enables us to capture aspects of the performance of our system that offline methods cannot. Whether offline evaluation, online evaluation or a combination of both is the best method to evaluate our system's performance still remains a topic of research. For the purposes of this notebook, we will only be covering simplistic forms of offline evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommenders\n",
    "We can now begin to understand the first sub-class of recommendation systems: __content-based recommenders.__ Let us look at the recommendation problem in the context of our movies dataset. It is intuitive to say that we would like to recommend romance movies to someone that has rated other romantic movies highly as opposed to action, or to recommend older films to users to who are fans of old classics, or even Batman movies to a Batman fan. In this context, we are looking at the characteristics (content) of each movie, and recommending movies that are similar to the previously highly rated movies by the same user. \n",
    "\n",
    "There are multiple approaches for the machinery of content-based recommenders. Most will either use the features of movies to predict whether you like or dislike a movie (classification) or to predict the rating the user would give to a movie they have not yet seen (__model-based__). Some might even use the features of a movie you have just watched and recommend the most similar movies to that given movie given their respective features (__memory-based__). We will be creating our own algorithm to predict the ratings of unseen movies and recommend those that are rated the highest.\n",
    "\n",
    "In modelling terms, we can frame the problem of recommendation as using the __features__ of movies watched by a user and the ratings given to each movie to __predict__ the rating the user would give to a movie not yet watched based on the movie's features. This is why this approach is referred to as item-centred. In other words, if we have enough data, we train a model for each user based on the previously watched movies and their features. The features we have chosen to use in our model are given genres, vote average, release date and runtime. While release date and vote average are available, we have to extract and process the genres for each movie. Given that we have a list of genres for each movie, we will have to dummy encode it as follows:\n",
    "- Determine how many genres there are and make each genre a feature of the model\n",
    "- Assign to each genre a zero if it is not present in the movie's list of genres, or 1 for each genre given each movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Importing our data\n",
    "links_small = pd.read_csv(\"../DATA/links_small.csv\")\n",
    "ratings_small = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "md = pd.read_csv(\"../DATA/movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying our data\n",
    "# md\n",
    "# ratings_small\n",
    "#links_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only considering movies with the upper 15% most votes\n",
    "m = md[\"vote_count\"].quantile(0.85)\n",
    "md = md[md[\"vote_count\"] > m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure imdb_id matches between links and metadata\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].str.strip(\"tt\")\n",
    "\n",
    "# Removing all movies without a genre, release_date or runtime\n",
    "md[\"genres\"].replace('[]', np.nan, inplace=True)\n",
    "md.dropna(subset=[\"genres\", \"release_date\", \"imdb_id\", \"vote_average\"], inplace=True)\n",
    "\n",
    "# Converting release_date to a POSIX timestamp float\n",
    "md[\"release_date\"] = pd.to_datetime(md[\"release_date\"], infer_datetime_format=True)\n",
    "md[\"release_date\"] = md[\"release_date\"].apply(lambda x:x.timestamp())\n",
    "\n",
    "# Converting imdb_id to int\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"genres\" column from a dictionary from a list of strings, containing the respective genres\n",
    "def extract_genres(x):\n",
    "    genre_string = ''\n",
    "    x = eval(x) # executes expression inside of string\n",
    "    for dictionary in x:\n",
    "        genre_string += dictionary[\"name\"] + '|'\n",
    "    return genre_string # include all but last one\n",
    "md[\"genres\"] = md[\"genres\"].apply(extract_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data we are going to use for this model, we can extract the three columns: genre, release_data and budget. We will now also determine the unique features present in the dataset and use each as a feature. Note that this model assumes that all possible genres are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'Romance', 'Crime', 'TV Movie', 'Documentary', 'Drama', 'Music', 'Comedy', 'Science Fiction', 'War', 'Horror', 'Adventure', 'Fantasy', 'Animation', 'Thriller', 'History', 'Mystery', 'Western', 'Family']\n"
     ]
    }
   ],
   "source": [
    "# Initialising our features matrix\n",
    "FEATURES = md[[\"imdb_id\", \"original_title\", \"release_date\", \"vote_average\"]]\n",
    "\n",
    "# Finding unique genre names\n",
    "GENRES = md[\"genres\"]\n",
    "unique_genres = list(set(GENRES.sum().split('|')[:-1])) # Don't include last element\n",
    "print(unique_genres)\n",
    "\n",
    "# Removing '|' from the end of each string\n",
    "GENRES = GENRES.apply(lambda x:x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding each genre as a feature\n",
    "extended_features = GENRES.str.get_dummies()\n",
    "\n",
    "# Horizontally stack our extended features and the original features\n",
    "FEATURES = FEATURES.merge(extended_features, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Normalizing non-categorical features\n",
    "scaler = MinMaxScaler()\n",
    "FEATURES[['release_date', 'vote_average']] = scaler.fit_transform(FEATURES[['release_date', 'vote_average']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our features for each movie are set, we need to create a list of users, where each user contains the features and ratings of each of the movies they rated. To link the users and their ratings to the movies, we will need to use the intermediary \"links\" table. Be careful! We have dropped a few of the movies in the original dataset when the required feature was not available. First we will merge the appropriate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Join: links JOIN FEATURES ON imdbId\n",
    "FEATURES = FEATURES.rename(columns={'imdb_id':'imdbId'}) # making column names match\n",
    "first_join = links_small.merge(FEATURES, on=\"imdbId\")\n",
    "\n",
    "# Second Join: ratings JOIN first_join on movieId\n",
    "data_matrix = ratings_small.merge(first_join, on=\"movieId\")\n",
    "\n",
    "# Delete unnecessary columns\n",
    "data_matrix.drop(['movieId', 'timestamp', 'imdbId', 'tmdbId'],axis='columns', inplace=True)\n",
    "\n",
    "# Converting ratings from float to ints for Logistic Regression\n",
    "# data_matrix[\"rating\"] = data_matrix[\"rating\"].apply(lambda x: 2*x).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging our dataframes, we will create a list of users, each with all the ratings provided by each user and features of the corresponding rated movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique user list\n",
    "unique_users = list(set(data_matrix[\"userId\"]))\n",
    "user_data = {}\n",
    "\n",
    "# Adding movie ratings and features to the list of the corresponding user for users that have rated more than 15 movies\n",
    "data_copy = data_matrix.copy()\n",
    "tr = 0 # at least n reviews\n",
    "for user_id in unique_users:\n",
    "    current_data = data_copy[data_copy[\"userId\"] == user_id]\n",
    "    if current_data.shape[0] >= tr:\n",
    "        user_data[user_id] = current_data\n",
    "    data_copy = data_copy[data_copy[\"userId\"] != user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 596.3974659442902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Train regression model on each user!\n",
    "user_models = {}\n",
    "scores = []\n",
    "start = time.time()\n",
    "for user_id in user_data.keys():\n",
    "    # Get user data\n",
    "    data = user_data[user_id]\n",
    "    Y = data[\"rating\"]\n",
    "    X = data.drop([\"userId\", \"rating\", \"original_title\"], axis=1)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    # Train model and predict on test data\n",
    "    reg = RandomForestRegressor(n_estimators=80, max_depth=3, random_state=0)\n",
    "    fitted_reg = reg.fit(X_train, Y_train)\n",
    "#     predictions = fitted_reg.predict(X_test)\n",
    "    scores.append(cross_validate(reg, X, Y, scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], cv=5))\n",
    "    user_models[user_id] = fitted_reg # storing estimator for this user!\n",
    "#     print(\"Y_test:\", Y_test, \"Predictions:\", predictions)\n",
    "\n",
    "print(\"elapsed time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(\"Random Forest Regressor:\")\n",
    "# print(\"R2:\", r2_score(Y_test, predictions))\n",
    "# print(\"MAE\", MAE(Y_test, predictions))\n",
    "# print(\"RMSE:\", MSE(Y_test, predictions, squared=False))\n",
    "# print(\"Number of reviews:\", len(Y_test)*5)\n",
    "# # print()\n",
    "# # print(\"Linear Regression:\")\n",
    "# # print(\"R2:\", r2_score(Y_test, predictions2))\n",
    "# # print(\"MAE\", MAE(Y_test, predictions2))\n",
    "# # print(\"RMSE:\", MSE(Y_test, predictions2, squared=False))\n",
    "# # print()\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.scatter(Y_test, predictions)\n",
    "# # plt.scatter(Y_test, predictions2)\n",
    "# plt.xlabel(\"Labels\")\n",
    "# plt.ylabel(\"Predictions\")\n",
    "# plt.legend((\"Random Forest\", \"Linear Regressor\"))\n",
    "# ax = plt.gca()\n",
    "# ax.set_ylim([0,10])\n",
    "# ax.set_xlim([0,10])\n",
    "# plt.show()\n",
    "mae = []\n",
    "rmse = []\n",
    "\n",
    "for dictionary in scores:\n",
    "    mae.append(np.mean(dictionary[\"test_neg_mean_absolute_error\"]))\n",
    "    rmse.append(np.mean(dictionary[\"test_neg_root_mean_squared_error\"]))\n",
    "print(\"Mean MAE:\", np.mean(mae))\n",
    "print(\"Mean RMSE:\", np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_user_id = random.choice(list(user_data.keys()))\n",
    "\n",
    "# Check the predictions of the recommender\n",
    "model = user_models[max_user_id]\n",
    "movies = FEATURES.iloc[:, 1:] #pd.concat([FEATURES.iloc[:, 1:], user_data[max_user_id].iloc[:, 2:]]).drop_duplicates(keep=False)\n",
    "\n",
    "# Rating all movies with the model trained on the one user\n",
    "predictions = pd.DataFrame({\"rating\": model.predict(movies.iloc[:, 1:])})\n",
    "rated_movies = predictions.merge(movies, left_index=True, right_index=True)\n",
    "\n",
    "# Diplay top 10 rated and top 10 recommendations!\n",
    "top_rated = user_data[max_user_id].nlargest(columns=\"rating\", n=10)\n",
    "top_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 recommendations\n",
    "rated_movies.nlargest(columns=\"rating\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean RMSE over all users:\", np.mean(rmse))\n",
    "print(\"Mean MAE over all users:\", np.mean(mae))\n",
    "print(\"Mean R-squared over all users:\", np.mean(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the evaluation metrics, we can see that the prediction made by our model deviates on average by 0.63 from the true rating, which is not a large deviation, and is generally able to distinguish between 'okay', 'great' and 'terrible' movies in the opinion of the user. However, if we would like to recommend 10 movies out of roughly 45,000, our rating predictions need to be even better. Therefore, this model may serve as a strong baseline model that can be improved by accounting for other features such as cast, directors, plot, amongst others.\n",
    "\n",
    "### Limitations:\n",
    "As you have seen, we only considered users that had reviewed 100 movies or more, as we do not have enough data to accurately train a model for that given user for a large number of features. What happens in the real-world if we need to make recommendations for users who have rated few movies? What if they are a completely new user? This is known as the __cold-start__ problem and makes it so that models such as our content-based one struggle to make recommendations for users whom we have little data on, making it an issue of __data sparsity__. Additionally, this approach requires feature extraction, which can sometimes be cumbersome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative-filtering\n",
    "\n",
    "The second classification of algorithms we are now going to go over is known as __collaborative-filtering__, which, as the name implies, requires the collaboration of all the different members of our user-base. It works under the assumption that users that have had similar preferences/choices in the past have similar tastes. If Susan and Tom both love the same movies, if Susan loves _He's Just Not That Into You_ then we should probably recommend that movie to Tom! Unlike content-based recommender systems, collaborative filtering does not require any manual feature extraction, hence why it's referred to as user-centred.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/676/1*_x4VqIeV9L6fxXm5PeAYsg.png\" width=\"500px\" height=\"500px\">\n",
    "\n",
    "As before, there are different approaches to how we can create a collaborative filtering system:\n",
    "- We can either look at the preferences/ratings of users who have similar interests and predict the preferences/ratings of this user (memory-based)\n",
    "- We can use the data we have on the preferences/ratings of all users in our dataset to __learn__ and/or __predict__ the features of individual different items and use these features to predict preferences/ratings for given items (model-based)\n",
    "\n",
    "As we have covered the model-based approach in the content-based recommendation section, we will be implementing a memory-based approach for collaborative filtering by using some of the unsupervised learning techniques we have encountered so far in the course. If you are interested in looking into an implementation that uses feature learning, check out this [video](https://www.youtube.com/watch?v=9AP-DgFBNP4&ab_channel=ArtificialIntelligence-AllinOne).\n",
    "\n",
    "More specifically, we will be creating a ratings vector for each user and using a KNN with means algorithm to compute mean rating of the user's $K$ nearest neighbours. We will be using the __Surprise library__, a scikit for building and analyzing recommender systems that deal with explicit rating data. Before we use our data we will have to clean it just as in the case for the content-based system. For the Surprise 'cross-validate' method, we need to have our data as three columns in the following order: userId, movieId, rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing our data\n",
    "links_small = pd.read_csv(\"../DATA/links_small.csv\")\n",
    "ratings_small = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "md = pd.read_csv(\"../DATA/movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering movie data by vote count\n",
    "md = md[md[\"vote_count\"] >= 50]\n",
    "md.dropna(subset=[\"imdb_id\", \"title\"], inplace=True)\n",
    "md.drop_duplicates(subset=[\"imdb_id\"], inplace=True)\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].str.strip(\"tt\").astype('int64')\n",
    "\n",
    "# Merging our data\n",
    "movies = md[[\"imdb_id\", \"title\"]]\n",
    "movies.rename(columns={'imdb_id':'imdbId'}, inplace=True) # making column names match\n",
    "first_join = links_small.merge(movies, on=\"imdbId\")\n",
    "joined_data = ratings_small.merge(first_join, on=\"movieId\")\n",
    "joined_data.drop(['title', 'timestamp', 'tmdbId', 'imdbId'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.knns import  KNNWithMeans\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "# joined_data\n",
    "\n",
    "data = Dataset.load_from_df(joined_data, reader)\n",
    "algo = KNNWithMeans()\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this algorithm actually doing? In memory-based approaches with algorithms such as KNN, we generally construct a __user-item matrix,__ as seen in the diagram above. This is a 2D representation of users vs items, such that each row contains all the ratings (or missing ratings) a user gave to every movie. Cells representing movies that have not been rated by the respective users are filled with zeros, NaNs or other placeholders. In the case of this surprise algorithm, they created a __similarity matrix__ containing the measure of cosine similarity between each user pair and uses that similarity as weights when computing the average rating of all its neighbours.\n",
    "\n",
    "Some of the most popular frameworks for collaborative filtering algorithms use __matrix factorization__ approaches, which include using a variant of SVD (as SVD cannot be applied to a matrix with missing data), known as SVD++. In fact, this is the algorithm popularized by Simon Funk during the Netflix prize for recommender systems! The theory of this algorithm extends beyond the scope of this course, but you can read more about it [here](https://www.hindawi.com/journals/mpe/2017/1975719/). Luckily this algorithm is also implemented by the Surprise library!\n",
    "\n",
    "[Documentation](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "# CF Algorithm using SVD++\n",
    "algo = SVD()\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
